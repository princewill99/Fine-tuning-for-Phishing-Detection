{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 24\n",
    "EMAIL = \"final_cleaned_text\"\n",
    "LABEL = \"label\"\n",
    "TOKENS = \"input_ids\"\n",
    "MASK = \"attention_mask\"\n",
    "GPT = \"gpt2\"\n",
    "BERT = \"bert-base-uncased\"\n",
    "ROBERTA = \"roberta-base\"\n",
    "DISTILBERT = \"distilbert-base-uncased\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# DEVICE = 'cpu'\n",
    "METRICS = \"metrics\"\n",
    "ACC = \"accuracy\"\n",
    "PR = \"precision\"\n",
    "RC = \"recall\"\n",
    "F1 = \"f1_score\"\n",
    "CM = \"conf_matrix\"\n",
    "TIME = \"time\"\n",
    "LOSS = \"loss\"\n",
    "TRAIN = \"training\"\n",
    "VAL = \"validation\"\n",
    "TEST = \"testing\"\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Data and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "        file:str=\"combined_file.csv\"\n",
    "):\n",
    "    data = []\n",
    "    for idx, row in pd.read_csv(file)[[EMAIL, LABEL]].iterrows():\n",
    "        if type(row[EMAIL]) is str:\n",
    "            data.append((\n",
    "                row[EMAIL], \n",
    "                np.array([row[LABEL]], dtype=np.float32)\n",
    "            ))\n",
    "    return pd.DataFrame(data, columns=[EMAIL, LABEL])\n",
    "\n",
    "def split_data(\n",
    "        data: pd.DataFrame,\n",
    "        train_size: float = 0.7,\n",
    "        val_size: float = 0.2\n",
    "):\n",
    "    data_len = data.shape[0]\n",
    "    train_idx = int(data_len * train_size)\n",
    "    val_idx = train_idx + int(data_len * val_size)\n",
    "\n",
    "    train_set = data.iloc[:train_idx, :]\n",
    "    val_set = data.iloc[train_idx:val_idx, :]\n",
    "    test_set = data.iloc[val_idx:, :]\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "        model_key,\n",
    "        data: pd.DataFrame\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_key,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    "    if model_key == GPT:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        data[EMAIL].tolist(),\n",
    "        padding=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return Dataset.from_dict({\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask'],\n",
    "        'labels': list(map(int, data[LABEL]))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create Model Specific Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_sets():\n",
    "    # Data Dictionary\n",
    "    result = {}\n",
    "    \n",
    "    # Load Base Set\n",
    "    shared_data = load_data()\n",
    "    # shared_data = shared_data.sample(n=10000, random_state=RANDOM_STATE)\n",
    "\n",
    "    tr, vl, ts = split_data(shared_data)\n",
    "\n",
    "    # Load Train/Validation/Test Sets\n",
    "    def add_model(key):\n",
    "        result[key] = {\n",
    "            TRAIN:tokenize(key, tr),\n",
    "            VAL:tokenize(key, vl),\n",
    "            TEST:tokenize(key, ts)\n",
    "        }\n",
    "    \n",
    "    # Add Model Sets\n",
    "    add_model(BERT)\n",
    "    add_model(ROBERTA)\n",
    "    add_model(DISTILBERT)\n",
    "    add_model(GPT)\n",
    "\n",
    "    return result\n",
    "\n",
    "data = create_model_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = eval_pred\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_pred, y_true)\n",
    "    pr = precision_score(y_pred, y_true, average='weighted')\n",
    "    rc = recall_score(y_pred, y_true, average='weighted')\n",
    "    f1 = f1_score(y_pred, y_true, average='weighted')\n",
    "\n",
    "    return {\n",
    "        ACC:acc,\n",
    "        PR:pr,\n",
    "        RC:rc,\n",
    "        F1:f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_trainer(model, key):\n",
    "    targs = TrainingArguments(\n",
    "        output_dir=f\"./results/{key}/\",\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs/{key}/\",\n",
    "        logging_steps=10,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=targs,\n",
    "        train_dataset=data[key][TRAIN],\n",
    "        eval_dataset=data[key][VAL],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_setup():\n",
    "\n",
    "    def to_clf(key):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(key, num_labels=2)\n",
    "        if key == GPT:\n",
    "            model.config.pad_token_id = model.config.eos_token_id\n",
    "        return model\n",
    "\n",
    "    def to_cuda(model):\n",
    "        if torch.cuda.is_available():\n",
    "            return model.cuda()\n",
    "        return model\n",
    "\n",
    "    def create(key):\n",
    "        return to_trainer(\n",
    "            model=to_cuda(to_clf(key)),\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        BERT:create(BERT),\n",
    "        ROBERTA:create(ROBERTA),\n",
    "        DISTILBERT:create(DISTILBERT),\n",
    "        GPT:create(GPT)\n",
    "    }\n",
    "\n",
    "llm = model_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Define Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confusion Matrix Dictionary\n",
    "cm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(key, is_pretrained):\n",
    "    y_pred, y_true, metrics = llm[key].predict(data[key][TEST])\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    suffix = \"PT\" if is_pretrained else \"FT\"\n",
    "    cm[f\"{key} {suffix}\"] = confusion_matrix(y_true, y_pred)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Evaluate Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_results = {\n",
    "    BERT:evaluate(BERT, True),\n",
    "    ROBERTA:evaluate(ROBERTA, True),\n",
    "    DISTILBERT:evaluate(DISTILBERT, True),\n",
    "    GPT:evaluate(GPT, True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finetune Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = {} # Dictionary to save TrainOutput object of each model\n",
    "\n",
    "def plot_loss(key):\n",
    "    \"\"\"Plot training/validation loss vs epoch\"\"\"\n",
    "    results = pd.DataFrame(llm[key].state.log_history)\n",
    "    plt.figure(figsize=(5,3))\n",
    "    n_epochs = int(train_output[key].metrics['epoch'])\n",
    "    x = [i+1 for i in range(n_epochs)]\n",
    "    plt.plot(x, results[\"loss\"].dropna().head(n_epochs))\n",
    "    plt.plot(x, results[\"eval_loss\"].dropna().head(n_epochs))\n",
    "    plt.title(f\"{key} Training/Validation Loss vs. Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[BERT] = llm[BERT].train()\n",
    "plot_loss(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[ROBERTA] = llm[ROBERTA].train()\n",
    "plot_loss(ROBERTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[DISTILBERT] = llm[DISTILBERT].train()\n",
    "plot_loss(DISTILBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[GPT] = llm[GPT].train()\n",
    "plot_loss(GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluate Finetuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = {\n",
    "    BERT:evaluate(BERT, False),\n",
    "    ROBERTA:evaluate(ROBERTA, False),\n",
    "    DISTILBERT:evaluate(DISTILBERT, False),\n",
    "    GPT:evaluate(GPT, False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create Metrics DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([\n",
    "    *pretrained_results.values(), *finetuned_results.values()\n",
    "], index=[\n",
    "    \"BERT PT\",\n",
    "    \"ROBERTA PT\",\n",
    "    \"DISTILBERT PT\",\n",
    "    \"GPT2 PT\",\n",
    "\n",
    "    \"BERT FT\",\n",
    "    \"ROBERTA FT\",\n",
    "    \"DISTILBERT FT\",\n",
    "    \"GPT2 FT\"\n",
    "\n",
    "]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot.bar(\n",
    "    y=[\"test_accuracy\"],\n",
    "    figsize=(5, 4),\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(\"Accuracy (%)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Precision, Recall, and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot.bar(\n",
    "    y=[\"test_precision\", \"test_recall\", \"test_f1_score\"], \n",
    "    figsize=(12,4),\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(\"Precision, Recall, and F1-Score\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix font \n",
    "font = {'family' : 'sans-serif',\n",
    "          'weight' : 'normal',\n",
    "          'size'   : 15}\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "def plot_cm(key):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(5, 3))\n",
    "    ConfusionMatrixDisplay(cm[f\"{key} PT\"]).plot(ax=ax[0], cmap=\"GnBu\", colorbar=False)\n",
    "    ConfusionMatrixDisplay(cm[f\"{key} FT\"]).plot(ax=ax[1], cmap=\"GnBu\", colorbar=False)\n",
    "    ax[0].set_title(\"Pretrained\")\n",
    "    ax[1].set_title(\"Finetuned\")\n",
    "    plt.suptitle(f\"    {\"BERT\"} Confusion Matrices\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(ROBERTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(DISTILBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(GPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
